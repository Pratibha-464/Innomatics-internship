{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTKuUOn7HMSu"
      },
      "outputs": [],
      "source": [
        "! pip install streamlit torch torchaudio pydub transformers langchain_google_genai langchain_core langchain_community chromadb sentence-transformers scikit-learn numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "s60ZQxRAHPO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app2.py\n",
        "import streamlit as st\n",
        "import torch\n",
        "import torchaudio\n",
        "import os\n",
        "import tempfile\n",
        "import numpy as np\n",
        "from pydub import AudioSegment\n",
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Load Whisper model from Hugging Face\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n",
        "whisper_model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\")\n",
        "\n",
        "# Load Gemini API Key\n",
        "with open(\"api.txt\", \"r\") as file:\n",
        "    GOOGLE_API_KEY = file.read().strip()\n",
        "\n",
        "# Initialize Gemini AI model\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", google_api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# Initialize ChromaDB\n",
        "persist_directory = \"/content/drive/MyDrive/files/chroma_db\"\n",
        "embeddings_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        "db = Chroma(collection_name=\"vector_database\", embedding_function=embeddings_model, persist_directory=persist_directory)\n",
        "\n",
        "# Retriever\n",
        "retriever = db.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "# Prompt Template\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"chat_history\", \"series_name\", \"reasoning\", \"question\"],\n",
        "    template=(\n",
        "        \"You are an AI assistant that provides direct answers based ONLY on the given context.\\n\\n\"\n",
        "        \"Previous Conversation History:\\n{chat_history}\\n\\n\"\n",
        "        \"Series Name: {series_name}\\n\\n\"\n",
        "        \"Relevant Events Leading to the Answer:\\n{reasoning}\\n\\n\"\n",
        "        \"User Question:\\n{question}\\n\"\n",
        "        \"Final Answer (No extra text, just the answer):\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# Chat History Storage\n",
        "chat_history = []\n",
        "\n",
        "# Function: Retrieve & Process Data\n",
        "def retrieve_and_process(inputs):\n",
        "    query = inputs[\"question\"]\n",
        "    retrieved_docs = retriever.invoke(query)\n",
        "\n",
        "    if not retrieved_docs:\n",
        "        return {\n",
        "            \"chat_history\": \"\",\n",
        "            \"series_name\": \"Unknown\",\n",
        "            \"reasoning\": \"No relevant events found.\",\n",
        "            \"question\": query\n",
        "        }\n",
        "\n",
        "    extracted_texts = [doc.page_content for doc in retrieved_docs]\n",
        "    series_names = [doc.metadata.get(\"series_name\", \"Unknown\") for doc in retrieved_docs]\n",
        "\n",
        "    # üîπ Generate Reasoning\n",
        "    reasoning_prompt = f\"Summarize the most relevant events from '{series_names[0]}' before answering: {query}.\\n\\nContext:\\n\" + \"\\n\".join(extracted_texts)\n",
        "    reasoning = llm.invoke(reasoning_prompt).content.strip()\n",
        "\n",
        "    formatted_chat_history = \"\\n\".join([f\"User: {entry['user']}\\nAssistant: {entry['assistant']}\" for entry in chat_history])\n",
        "\n",
        "    return {\n",
        "        \"chat_history\": formatted_chat_history,\n",
        "        \"series_name\": series_names[0],\n",
        "        \"reasoning\": reasoning,\n",
        "        \"question\": query\n",
        "    }\n",
        "\n",
        "# RAG Chain\n",
        "  retrieval_chain = (\n",
        "      RunnableLambda(retrieve_and_process)\n",
        "      | (lambda x: {**x, \"answer\": llm.invoke(prompt_template.format(**x)).content.strip()})  # Ensure dictionary output\n",
        "  )\n",
        "\n",
        "# Function: Process Audio & Transcribe using Hugging Face Whisper\n",
        "def process_audio(audio_file):\n",
        "    temp_audio_path = tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\").name\n",
        "\n",
        "    # Convert to 16kHz WAV\n",
        "    audio = AudioSegment.from_file(audio_file)\n",
        "    audio = audio.set_frame_rate(16000).set_channels(1)\n",
        "    audio.export(temp_audio_path, format=\"wav\")\n",
        "\n",
        "    # Load audio\n",
        "    speech, sr = torchaudio.load(temp_audio_path)\n",
        "    resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)\n",
        "    speech = resampler(speech).squeeze()\n",
        "\n",
        "    # Process the input audio\n",
        "    input_features = processor(speech, sampling_rate=16000, return_tensors=\"pt\").input_features\n",
        "\n",
        "    # Generate transcription\n",
        "    with torch.no_grad():\n",
        "        predicted_ids = whisper_model.generate(input_features)\n",
        "\n",
        "    # Decode and return transcription\n",
        "    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "    # Remove temp file\n",
        "    os.remove(temp_audio_path)\n",
        "\n",
        "    return transcription\n",
        "\n",
        "# Function: Ask Question (Retrieve & Generate Response)\n",
        "def ask_question(user_query):\n",
        "    global chat_history\n",
        "\n",
        "    result = retrieval_chain.invoke({\"question\": user_query})\n",
        "\n",
        "    # Result is a dictionary\n",
        "    if not isinstance(result, dict):\n",
        "        raise TypeError(f\"Unexpected output from retrieval chain: {result}\")\n",
        "\n",
        "    # Extract fields safely\n",
        "    answer = result.get(\"answer\", \"No answer found.\")\n",
        "    series_name = result.get(\"series_name\", \"Unknown\")\n",
        "    reasoning = result.get(\"reasoning\", \"No reasoning found.\")\n",
        "\n",
        "    # Store in Chat History\n",
        "    chat_history.append({\n",
        "        \"user\": user_query,\n",
        "        \"assistant\": answer,\n",
        "        \"series_name\": series_name,\n",
        "        \"reasoning\": reasoning\n",
        "    })\n",
        "\n",
        "    return series_name, reasoning, answer\n",
        "\n",
        "# Streamlit UI\n",
        "st.title(\"üîç AI-Powered Video Subtitle Search\")\n",
        "\n",
        "# Upload Audio File\n",
        "uploaded_file = st.file_uploader(\"üìÇ Upload an audio file\", type=[\"mp3\", \"wav\", \"w4a\", \"m4a\", \"ogg\", \"flac\"])\n",
        "\n",
        "# Manual Text Input\n",
        "user_input = st.text_area(\"‚úçÔ∏è Or manually enter text\")\n",
        "\n",
        "if uploaded_file:\n",
        "    st.audio(uploaded_file, format=\"audio/wav\")\n",
        "    if st.button(\"üé§ Transcribe & Search\"):\n",
        "        with st.spinner(\"Processing audio...\"):\n",
        "            transcribed_text = process_audio(uploaded_file)\n",
        "        st.subheader(\"üìù Transcription\")\n",
        "        st.write(transcribed_text)\n",
        "        with st.spinner(\"üîé Searching for relevant subtitles...\"):\n",
        "            series_name, reasoning, answer = ask_question(transcribed_text)\n",
        "        st.subheader(\"üìå Search Results\")\n",
        "        st.write(f\"**üé¨ Series Name:** {series_name}\")\n",
        "        st.write(f\"**üìú Relevant Events:** {reasoning}\")\n",
        "        st.write(f\"**‚úÖ Answer:** {answer}\")\n",
        "\n",
        "if st.button(\"üîç Submit\"):\n",
        "    if user_input.strip():\n",
        "        with st.spinner(\"üîé Searching for relevant subtitles...\"):\n",
        "            series_name, reasoning, answer = ask_question(user_input)\n",
        "        st.subheader(\"üìå Search Results\")\n",
        "        st.write(f\"**üé¨ Series Name:** {series_name}\")\n",
        "        st.write(f\"**üìú Relevant Events:** {reasoning}\")\n",
        "        st.write(f\"**‚úÖ Answer:** {answer}\")\n",
        "    else:\n",
        "        st.warning(\"‚ö†Ô∏è Please enter some text before submitting.\")\n"
      ],
      "metadata": {
        "id": "cM_bAMcqHRvB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}