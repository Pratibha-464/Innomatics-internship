# -*- coding: utf-8 -*-
"""Intialization of DB.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cDmGdmYlDM6yyQXOR60zK01g9XTns0OL
"""

! pip install --upgrade pip

! pip uninstall numpy

! pip install numpy==1.26.4

!pip install torch==1.11.0+cpu torchvision==0.12.0+cpu torchaudio==0.11.0+cpu --extra-index-url https://download.pytorch.org/whl/cpu

! pip install transformers==4.34.0

! pip install scikit-learn scipy nltk

! pip install --force-reinstall numba

! pip install "numpy==1.26.4" "numba==0.60.0"

! pip install jedi

! pip install langchain-huggingface

! pip install numpy transformers

! pip install "numpy<2.0"

pip install --upgrade numpy

! pip install -U sentence-transformers

! pip install --upgrade huggingface_hub

! pip install transformers==4.37.2 peft==0.10.0

! pip install -U langchain-chroma

pip install langchain langchain_huggingface chromadb tqdm scikit-learn scipy

! pip uninstall numpy
! pip install --no-cache-dir numpy

! pip install --upgrade --force-reinstall --no-cache-dir numpy scipy scikit-learn

import numpy
import torch
import transformers
import sklearn
import scipy
import nltk

print(f'NumPy version: {numpy.__version__}')
print(f'PyTorch version: {torch.__version__}')
print(f'Transformers version: {transformers.__version__}')
print(f'Scikit-learn version: {sklearn.__version__}')
print(f'SciPy version: {scipy.__version__}')
print(f'NLTK version: {nltk.__version__}')

import pandas as pd
import sqlite3
import zlib
import zipfile
import io
import chardet
import re
from langchain_chroma import Chroma
import os

from google.colab import drive
drive.mount('/content/drive')

"""Accessing Database"""

# Read the code below and write your observation in the next cell

conn = sqlite3.connect('/content/drive/MyDrive/Copy of eng_subtitles_database.db')
cursor = conn.cursor()
cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
print(cursor.fetchall())

cursor.execute("PRAGMA table_info('zipfiles')")
cols = cursor.fetchall()
for col in cols:
    print(col[1])

"""Converting into CSV"""

query = """
SELECT num, name, content
FROM zipfiles
ORDER BY RANDOM()
LIMIT (SELECT CAST(COUNT(*) * 0.3 AS INT) FROM zipfiles);
"""
df = pd.read_sql_query(query, conn)


# Close the database connection
conn.close()

# Display DataFrame
print(df.head())

"""Encoding"""

count = 0

def decode_method(binary_data):
    """Extracts and properly decodes subtitle files from a ZIP archive."""
    global count
    count += 1

    # Read ZIP file
    with io.BytesIO(binary_data) as f:
        with zipfile.ZipFile(f, 'r') as zip_file:
            # Extract the first file inside ZIP
            subtitle_content = zip_file.read(zip_file.namelist()[0])

    # Detect encoding dynamically
    detected_encoding = chardet.detect(subtitle_content)["encoding"]

    if detected_encoding is None:
        detected_encoding = "latin-1"  # Fallback to Latin-1

    # Decode the text using detected encoding
    text = subtitle_content.decode(detected_encoding, errors="ignore")

    # Remove BOM if present
    text = text.lstrip("\ufeff")

    # Remove leading numbers and newlines
    import re
    text = re.sub(r'^\d+\s+', '', text)  # Remove leading numbers
    text = text.replace("\r", "").replace("\n", " ").strip()  # Normalize spaces

    return text

"""Dataframe into CSV"""

df.to_csv("/content/drive/MyDrive/files/subtitles.csv", index=False)

"""Load CSV"""

# Load CSV
df = pd.read_csv("/content/drive/MyDrive/files/subtitles.csv")

"""Preporcessing"""

word = "Script Info"
df_filtered = df[df["cleaned_text"].str.contains(rf"\b{word}\b", case=False, na=False)]
print(df_filtered)

def clean_ass_subtitles(text):
    """Extracts only dialogue from ASS subtitle content and removes all metadata."""

    if isinstance(text, bytes):  # If text is binary, decode it first
        text = text.decode("utf-8", errors="ignore")

    # Extract only the spoken dialogue (ignoring metadata)
    matches = re.findall(r"Dialogue:\s\d+,\d{1,2}:\d{2}:\d{2}\.\d{2},\d{1,2}:\d{2}:\d{2}\.\d{2},[^,]*,[^,]*,\d+,\d+,\d+,,(.*)", text)

    # Clean up extracted text
    dialogues = []
    for match in matches:
        cleaned_text = re.sub(r"\{.*?\}", "", match).strip()  # Remove formatting tags like {an5}
        cleaned_text = cleaned_text.replace("\\N", " ")  # Replace line breaks
        cleaned_text = re.sub(r"\d{1,2}:\d{2}:\d{2}\.\d{2}", "", cleaned_text)  # Remove timestamps
        cleaned_text = re.sub(r"Default,\w*,\d+,\d+,\d+,", "", cleaned_text)  # Remove style metadata
        cleaned_text = re.sub(r"Dialogue:\s*\d+.*?,", "", cleaned_text)  # Remove any reoccurring "Dialogue:"
        cleaned_text = re.sub(r"Dialogue\s*\d+.*?,", "", cleaned_text)  # Remove any reoccurring "Dialogue"
        cleaned_text = re.sub(r"\bUnknown,\d+,\d+,\d+,", "", cleaned_text)  # Remove "Unknown,0000,0000,0000,"
        cleaned_text = re.sub(r"\b\d+,\d+,\d+,\d+,,", "", cleaned_text)  # Remove similar numeric metadata patterns
        cleaned_text = re.sub(r",\s*,+", " ", cleaned_text).strip() # Removes metadata and replaces multiple consecutive commas with a single space.

        if cleaned_text:
            dialogues.append(cleaned_text)

    return " ".join(dialogues)  # Return the cleaned dialogue

df_filtered["cleaned_text"] = df_filtered["cleaned_text"].astype(str).apply(clean_ass_subtitles)

df_filtered

word = "Script Info"
df_filtered_1 = df[~df["cleaned_text"].str.contains(rf"\b{word}\b", case=False, na=False)]
print(df_filtered_1)

import re
import pandas as pd

def clean_srt_subtitles(text):
    if pd.isna(text):  # Handle NaN values
        return ""

    # Remove timestamps (00:00:15,520 --> 00:00:18,580)
    text = re.sub(r"\d{2}:\d{2}:\d{2},\d{3} --> \d{2}:\d{2}:\d{2},\d{3}", "", text)

    # Remove sequence numbers (lines that contain only numbers)
    text = re.sub(r"^\d+\s*$", "", text, flags=re.MULTILINE)

    # Remove HTML tags (if any)
    text = re.sub(r"<[^>]+>", "", text)

    # Remove \n (if any)
    text = re.sub(r"\n\n|\n", " ", text)

    # Replace multiple spaces and newlines with a single space
    text = re.sub(r"\s+", " ", text).strip()

    # Remove line numbers (if present)
    text = re.sub(r"^\d+\s*$", "", text, flags=re.MULTILINE)

    # Remove numbers only if they are at the start of a line
    text = re.sub(r"\b\d+\s*", "", text, flags=re.MULTILINE)

    # Remove backslashes before apostrophes (e.g., "I\'m" -> "I'm")
    text = text.replace(r"\'", "'")

    return text

df_filtered_1["cleaned_text"] = df_filtered_1["cleaned_text"].apply(clean_srt_subtitles)

df_filtered_1

# Assuming df1 and df2 have the same columns
df = pd.concat([df_filtered, df_filtered_1], ignore_index=True)

df

df["cleaned_text"] = df["cleaned_text"].str.lower()

import re
common_spam = [
    "support us and become vip member  to remove all ads from www.opensubtitles.org",
    "api.OpenSubtitles.org is deprecated, please implement REST API from OpenSubtitles.com",
    "Use the free code JOINNOW at â€¨www.playships.eu"

]

def remove_repeated_phrases(text):
    for phrase in common_spam:
        text = re.sub(re.escape(phrase), "", text, flags=re.IGNORECASE)
    return text.strip()

df["cleaned_text"] = df["cleaned_text"].apply(remove_repeated_phrases)

df

# Replace dots with spaces except inside parentheses
df['name'] = df['name'].str.replace(r'\.', ' ', regex=True)

# Capitalize each word properly
df['name'] = df['name'].str.title()

# Print the cleaned names
print(df)

"""Chunking"""

import pandas as pd
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document

# Define Chunking Strategy
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=100
)

# List to store chunks
chunk_data = []

# Process each series separately
for name, group in df.groupby("name"):  # Group by series/movie name
    docs = [Document(page_content=text, metadata={"name": name}) for text in group["cleaned_text"]]

    # Split subtitles into chunks
    chunks = text_splitter.split_documents(docs)

    # Store chunks with correct series name
    for chunk in chunks:
        chunk_data.append({"chunk_content": chunk.page_content, "name": chunk.metadata["name"]})

# Convert to DataFrame
df_chunks = pd.DataFrame(chunk_data)

# Display result
print(df_chunks.head())

"""Embbeding"""

from langchain_huggingface import HuggingFaceEmbeddings
import numpy as np

model_name = "sentence-transformers/all-mpnet-base-v2"

embeddings_model = HuggingFaceEmbeddings(model_name=model_name)

"""Intializing Vector Database"""

# Define the directory to persist the Chroma database
persist_directory = "/content/drive/MyDrive/files2/chroma_db"

# Initialize the Chroma database connection
db = Chroma(collection_name="vector_database",
            embedding_function=embeddings_model,
            persist_directory=persist_directory)

# Iterate over DataFrame rows to add data to Chroma
for index, row in df_chunks.iterrows():
    series_name = row['name']
    content = row['chunk_content']

    # Add the document to the Chroma database
    db.add_texts(texts=[content], metadatas=[{"series_name": series_name}], ids=[str(index)])

# database

db.get()